---
title: "Introduction to surtvep"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{surtvep}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: sentence
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction:

`surtvep` is an R package for fitting penalized Newton's method for the time-varying effects model using mAIC, TIC, GIC as information criteria, in particular we span the parameter using basis functions.
Utilities for carrying out post-fitting visualization, summarization, and inference are also provided.
In this tutorial we introduce the use of `surtvep` through an example dataset.

## Installation:

```{r ,include=TRUE,eval=FALSE}
#Install the package, need to install the devtools packages:
require("devtools")
require("remotes")
remotes::install_github("UM-KevinHe/surtvep", ref = "Lingfeng_test")

#To install with Vignettes:
# install.packages("devtools")
# devtools::install_github("UM-KevinHe/surtvep",build_vignettes =T)
```

## Quick Start

The purpose of this section is to give users a general sense of the package.
We will briefly go over the main functions, basic operations and outputs.
After this section, users may have a better idea of what functions are available, which ones to use, or at least where to seek help.

First, we load the 'surtvep' package:

```{r}
library("surtvep")
```

The main functions used in the package are Newton's method 'coxtv' and Newton's method combined with penalization 'coxtp', which we will demonstrate in this section.
We load a set of data created beforehand for illustration:

```{r}
data("ExampleData")
z     <- ExampleData$x
time  <- ExampleData$time
event <- ExampleData$event
```

The command loads an input covariate matrix 'z', time-to-event outcome 'time' and 'event' from this saved R data archive.
The saved data set is a simulation data set with continuous outcomes.

We fit the Newton's method without penalization use the most basic call to 'coxtv'.

```{r}
fit.tv <- coxtv(z = z, event = event, time=time)
```

'fit.tv' is an object of class 'coxtv' that contains all the relevant information of the fitted model for further use.
We do not encourage users to extract the components directly.
Instead, various methods are provided for the object such as `plot` and `test` that enable us to execute those tasks more elegantly.

We can visualize the time-varying coefficients by executing the `plot` method:

```{r, fig = TRUE, fig.width=7, fig.height=3.5, warning = FALSE}
plot(fit.tv, ylim = c(-2,2))
```


Each sub figure corresponds to a variable. It shows the time-varying effect of our predictors. In our `ExampleData`, the first predictor has a constant effect of 1, and the second predictor has a time-varying effect of $\text{sin}(3\pi * t/4)$, where $t$ is the time. The dotted line indicates the that hazard ratio is 0, which means the predictor has no effect. Users may also wish to plot the effect of different covariates in the same plot: this can be done by setting `allinone = TRUE` in the plot command.



Next we fit the Newton's method combined with penalization method. We specify a range of penalization coefficients first, then call the `coxtp` function. Detailed disucussion of how to specify the range of penalization coefficients and how to choose the appropriate one will be discussed in section Information Criteria.

```{r, fig = TRUE}
lambda_all <- c(1)
fit.penalize = coxtp(z = z, event = event, time=time, lambda_spline = lambda_all, method = "ProxN", btr = "static")
```


```{r, fig = TRUE, fig.width=7, fig.height=3.5, warning = FALSE}
plot(fit.penalize, IC = "TIC", ylim = c(-2,2))
```


With the tools introduced so far, users are able to fit the time-varying model. There are many more arguments in the package that give users a great deal of flexibility. To learn more, move on to later section.


## Newton's method

In this section we introduce the Newton's method for estimating time-varying effects in detail.

Let $D_{i}$ denote the time lag from transplantation to death and $C_{i}$ be the
censoring time for patient $i$,
 $i=1,\ldots, n$. Here $n_j$ is the sample size. The observed
time is $T_{i} = \min\{D_{i},C_{i}\}$, and
 the death indicator is given by $\delta_{i} = I(D_{i} \leq C_{i})$.  Let
  $\boldsymbol{X}_{i}=(X_{i1}, \ldots, X_{iP})^T$ be a $P$-dimensional covariate
vector.  We assume that $D_{i}$ is independent from $C_{i}$ given $\textbf{X}_{i}$.
Consider the hazard function
\[
   \lambda(t|\boldsymbol{X}_{i}) = \lambda_{0}(t)\exp\{\boldsymbol{X}_{i}^T {\boldsymbol\beta}(t)\}, %\nonumber
\]
where $\lambda_{0}(t)$ is the baseline hazard.
 To estimate the time-varying coefficients ${\boldsymbol\beta}(t)=\{\beta_{1}(t),\ldots,  \beta_{P}(t)\}$, we span
 $\boldsymbol\beta(\cdot)$  by a set of  cubic B-splines defined on a given number of knots:
\begin{eqnarray}
   \beta_{p}(t)=\boldsymbol\theta_{p}^T  \boldsymbol{B}(t)=\sum_{k=1}^K \theta_{pk} B_k(t), ~~ p=1, \ldots, P,   \nonumber
\end{eqnarray}
where   $\boldsymbol{B}(t)=\{B_1(t), \ldots,
B_K(t)\}^T$ forms a basis,  $K$ is the number of basis functions, and
 $\boldsymbol\theta_{p}=(\theta_{p1}, \ldots, \theta_{pK})^T$ is a vector of coefficients with
 $\theta_{pk}$ being the coefficient for the $k$-th basis of the $p$-th
covariate.

With a length-$PK$  parameter vector
$\boldsymbol\theta=vec(\boldsymbol\Theta)$,  the vectorization of the coefficient matrix
$\boldsymbol\Theta=(\boldsymbol\theta_{1}, \ldots, \boldsymbol\theta_{P})^T$ by row,
 the log-partial likelihood function is
\begin{equation}
  \ell(\boldsymbol\theta)=\sum_{i=1}^{n_j} \delta_{i} \left [\boldsymbol{X}_{i}^T \boldsymbol\Theta  \boldsymbol{B}(T_{i})
  -\log
  \left\{\sum_{i' \in R_{i}}  \exp \{\boldsymbol{X}_{i' }^T \boldsymbol\Theta  \boldsymbol{B}(T_{i}) \} \right \} \right ]
\end{equation},
 where $R_{i}=\{i': 1 \leq i' \leq n, ~ T_{i'}\geq T_{i}\}$ is the at-risk set.

`coxtv` applies Newton's method to solve the problem. Specifically, suppose we have current estimates $\widetilde{\boldsymbol\theta}$, the update is
\[
    \widetilde{\boldsymbol\theta} \leftarrow \widetilde{\boldsymbol\theta} + \nu \boldsymbol{\mu};
\]
where
\[
  \boldsymbol{\mu} = \left(- \nabla^2 \ell(\boldsymbol{\theta}) \right)^{-1} \nabla \ell(\boldsymbol{\theta})
\],
and $\nu$ is a step size adjusted by backtracking linesearch. $\nabla \ell(\boldsymbol{\theta})$ and $\nabla^2 \ell(\boldsymbol{\theta})$ is the first and second derivative of the log partial likelihood.

### Commonly used function arguments

`coxtp` provides various arguments for users to customize the fit: we introduce some commonly used arguments here.

* `strata` is for the stratified cox model. stratification group defined in the data. If there exist stratification group, please enter as vector.

* `nsplines` is for the number of basis functions in the B-splines, denoted as $K$ in our model introduction.

* `ties` is for dealing with ties, default is **`ties="Breslow"`**, which uses brewslow approximation.

* `tol` is the convergence threshold. The default threshold is set as **`tol=1e-6`**.

* `iter.max` is the maximum Iteration number, default is **`iter.max=20L`**

* `method` is for selecting two variants if Newton's Method. **`method="Newton"`** uses the classical Newton's method. **`method="ProxN"`** is the default method, which uses the proximal Newton's methods.

* `lambda` is the parameter for Proximal Newton's Method. Default is **`lambda=1e8`**

* `btr` is for selecting backtracking line search approach, default is `btr="dynamic"`. When predictors are presented with low frequency, `btr="static"` can be used to improve stability.

* `tau` is a parameter in the backtracking line search to control the step size. Default is **`tau=0.5`**.

* `stop` is for selecting stopping rule, default is **`stop="ratch"`**:

* `parallel` is a logical flag for parallel computation, Default is **`parallel=FALSE`**.

* `threads` is the number of threads used when parallel computation is enabled (**`parallel=TRUE`**).

* `degree`

* `ord`

* `fixedstep` is a logical flag for fixed iteration steps. Default value is **`fixedstep = FALSE`**. If **`fixedstep = FALSE`**, the algorithm will be stopped by `iter.max` regardless the `stop` criteria is used.

In the following sections we brefily describe these useful arguments when calling `coxtv`.

Now we start with a relatively harsh simulated data. Here, the covariates V1 and V2 were generated as binary variables with around 90% frequency, which is a relatively harsh setting to be estimated.
The related true log-hazard function for each variable is $\beta(t)=1$ and $\beta(t)=exp(-1.5*t)$, where t denotes time.

Let's check the data first:

```{r}
data("ExampleDataBinary")
table(ExampleDataBinary$x[,1])
table(ExampleDataBinary$x[,2])

z     <- ExampleDataBinary$x
time  <- ExampleDataBinary$time
event <- ExampleDataBinary$event
```
Both predictors are presented with frequency around 25%.

### Proximal Newton's method: `method = "ProxN"`

The 'method' parameter has two options. **`method="Newton"`** and **`method="ProxN"`**.
The proximal Newton's method modified the second order derivative $\nabla^2 \ell(\boldsymbol{\theta})$ by adding small terms $1/\lambda$ to the the diagonal elements. The default value of $\lambda$ is $10^8$, which can be modified by user. If the data set have predictors with extremely low frequency, users may consider a smaller $\lambda$.

```{r, message = FALSE, results='hide'}
fit.newton <- coxtv(z = z, event = event, time=time, method = 'Newton')
fit.proxN <- coxtv(z = z, event = event, time=time, method = 'ProxN')
```

Then we plot the fitted methods with all the curves on the same plot. No obvious change can be oberserved here. However, we do recommand using 
```{r,  fig.ncol = 1, fig.show="hold", out.width="40%",out.height="44%", message = FALSE, warning=FALSE}
plot(fit.newton, ylim = c(-2,2), allinone = TRUE)
plot(fit.proxN, ylim = c(-2,2), allinone = TRUE)
```


### Stratified Newton's Method: `strata`

When different facilities are present, we can extend the model to stratified version. We use $j=1,\ldots,J$ to denote the $J$ different centers
Let $D_{ij}$ denote the time lag to death and $C_{ij}$ be the
censoring time for patient $i$ in center $j$,
 $i=1,\ldots, n_j$, and $j=1, \ldots, J$. Here $n_j$ is the sample size in center $j$. The total number of patients is  $N=\sum_{j=1}^Jn_j$, the observed time is $T_{ij} = \min\{D_{ij},C_{ij}\}$, and the death indicator is given by $\delta_{ij} = I(D_{ij} \leq C_{ij})$.  
 Let $\textbf{X}_{ij}=(X_{ij1}, \ldots, X_{ijP})^T$ be a $P$-dimensional covariate
vector.  We assume that $D_{ij}$ is independent from $C_{ij}$ given $\textbf{X}_{ij}$.
Correspondingly, the log-partial likelihood function is
\[
    \ell_{strata}(\boldsymbol\theta) = \sum_{j=1}^J \sum_{i=1}^{n_j} \delta_{ij} \left [\boldsymbol{X}_{ij}^T \boldsymbol\Theta  \boldsymbol{B}(T_{ij})
    -\log
    \left\{\sum_{i' \in R_{ij}}  \exp \{\boldsymbol{X}_{i' j}^T \boldsymbol\Theta  \boldsymbol{B}(T_{ij}) \} \right \} \right ],
\]
 where $R_{ij}=\{i': 1 \leq i' \leq n_j, ~ T_{i' j}\geq T_{ij}\}$ is the at-risk set for stratum $j$. 

For the case with different stratums, usage is to include `strata` variable. 
First, we load a set of generated data with different stratums. In our simulation, we vary the baseline for different stratums by adding a small term generated by uniform distribution with mean = 0 and standard deviation= 0.5.

```{r}
data("StrataExample")
z     <- StrataExample$x
time  <- StrataExample$time
event <- StrataExample$event
strata <- StrataExample$strata
```

The `strata` variable can be a `ns >= 1` level factor, numerical numbers or strings. The stratified model can be easily fitted by calling
```{r, message = FALSE, results='hide', fig = TRUE, fig.width=7, fig.height=3.5, warning = FALSE}
fit.strata <- coxtv(z = z, event = event, time=time, strata = strata)
plot(fit.strata, ylim = c(-2,2))
```


### Step size adjustment: `btr`





#### Model results meanings:

First, Let's look at the `fit` result

```{r}
# summary(fit.tv)
```

There are 4 results saved under the fit result.

-   `model_result` save the detailed model results which we will explain in a minute.

-   `lambda.selected` saved the best lambda chosen based on different criteria which will not be used for the non-penalized model.

-   `p` refers to the number of covariates used in the model

-   `z_names` records the covariates names.

The detailed model result was saved in `model_result`, which can be called by `fit$model_result`.
Now, let's explore the result a little bit:


Here we noticed that there are 16 items in the model results list.
Following is an explanation of each item:

-   `theta`: Estimation matrix of $\theta$
-   `logplkd`: log-partial likelihood
-   `theta.all`: Internal validation use
-   `theta.list`: The estimation matrix at each Newton's update
-   `AIC.all`: Akakia information criterion
-   `TIC.all`: Takeuchi information criterion
-   `GIC.all`: Generalized information criterion
-   `AIC.trace`: Internal validation use
-   `TIC.trace`: Internal validation use
-   `GIC.trace`: Internal validation use
-   `logplkd.vec`: log-partial likelihood at each iteration
-   `SplineType`: spline used for fitting the model
-   `VarianceMatrix`: Variance Matrix
-   `uniqfailtimes`: The input unique event times if `ties="Breslow"`, input time points if `ties="None"` (Add link)
-   `bases`: The basis function used for estimating time-varying effects
-   `knots`: Number of basis functions used for estimating time-varying effects

#### 4.1.2.2: How to get the effect of a specific time point?

We are more interested in estimating time-varying effect of the covariates.
Following is an simple tutorial of how to do that.
First, a little background about the time-varying effect in cox model(You could also check this part at our paper which have more detailed explanation.(insert link))

If we Let $X_i=(X_{i1},X_{i2},...X_{ip})^T$ refers to the $i_{th}$ individuals in the dataset with p covariates(which could also be understand as the $i_{th}$ row in the data we extract above).
Let $\lambda(t|X_i)$ denote the hazard of having the event at time t for the $i_{th}$ individual, $\lambda_0(t)$ denote the hazard of having the event at time 0.
When we considering the covarites having time fixed effect, we have the following formula for $\lambda(t|X_i)$:

$\lambda(t|X_i)=\lambda_0(t)exp(X_i^T\beta)$

Where $\beta$ refers to the coefficients where $\beta=(\beta_1,\beta_2,...\beta_p)$, which in this example, is (V1, V2).
Which have similar format as the GLM model.
For time-varying effect model, we are simply replace $\beta$ with a set of $\beta(t)$.
Thus, the time varying equations could be transferred as following:

$\lambda(t|X_i)=\lambda_0(t)exp(X_i^T\beta(t))$

Similar, $\beta(t)=(\beta_1(t),\beta_2(t),...\beta_p(t))$ where $\beta(.)$ refers to a set of cubic B-spline(Details for B-spline refers to here : insert link).
Where, the single $\beta_p(t)$ could be estimated using the following formula:

$\beta_p(t)=\theta_p^TB(t)=\sum_{k=1}^K\theta_{pk}B_k(t)$

Here, K refers to the given number of knots.

Thus, to calculate the time varying effect of coefficient p, we just need to get both estimated B spline and the $\theta$ matrix.
The B-spline was saved in `model_result$bases` and $\theta$ matrix was saved in the last item in `model_result$theta_list` Following is the code for calculation:

As a result, $\beta$ is a 2487\*2 matrix(we are using the "Breslow" ties, thus there are 2487 rows instead of 5000, detail about ties and Breslow ties could refers to here(insert link)).

We could also get the 95%CI for the estimation by the ``confint.surtiver``
```{r}
CI <- confint.surtiver(fit.proxN)
head(CI$tvef$X1)
```




## Newton Method with penalization:

The proximal Newton can help improve the estimation when the origional hessian matrix is very close to a singular one, which may often occur in the setting of time-varying effects, however, over-fitting issue still exists. We further improve the estimation by introducing the penalty. The basic idea of penalization is to control the model's smoothness by adding a 'wiggliness' penalty to the fitting objective. Rather than fitting the non-proportional hazards model by maximizing original log-partial likelihood,
it could be fitted by maximizing
\begin{align}
    \ell(\boldsymbol{\theta}) - P_\lambda(\boldsymbol{\theta}).
\end{align}

We have different choices of $P_\lambda(\boldsymbol{\theta})$. Potential choices are to use P-splines, and discrete penalties. Detailed discussions are provided below.

### P-spline

The P-splines are low rank smoothers using a B-spline basis, usually defined on evenly spaced knots, with a difference penalty applied directly to the parameters $\theta_{pk}$, to control function wiggliness.
When we set standard cubic B-spline basis functions,  the penalty function used for $\beta_p$ will be
\begin{align*}
    P_\lambda(\boldsymbol{\theta}) = \lambda\sum_{j=1}^P\sum_{k=1}^{K-1}\{(\boldsymbol{\theta}_{j(k+1)}  - \boldsymbol{\theta}_{jk})\}^2.
\end{align*}

It is straight forward to express the penalty as a quadratic form, $\boldsymbol{\theta}^T\boldsymbol{S}\boldsymbol{\theta}$, in this basis coefficients:

\begin{align*}
    \sum_{k=1}^{K-1}\{(\boldsymbol{\theta}_{j(k+1)}  - \boldsymbol{\theta}_{jk})\}^2
    =
        \boldsymbol{\theta}^T
    \begin{bmatrix}
        1 & -1 & 0 & 0 & . & . & . \\
        -1 & 2  & -1 & 0 & 0 & . & . \\
        0 & -1  & 2 & -1 & 0 & 0 & . \\
        . & . & . & . & . & . & .\\
        . & . & . & . & . & . & .\\
    \end{bmatrix}
    \boldsymbol{\theta}
\end{align*}

Hence the penalized fitting problem is to maximize 
\begin{align}
    \ell(\boldsymbol{\theta}) - \lambda\boldsymbol{\theta}^T\boldsymbol{S}\boldsymbol{\theta}
\end{align}
with respect to $\boldsymbol{\theta}$.

### Smoothing-spline

The reduced rank spline smoothers with derivative based penalties can be set up almost as easily, while retaining the sparsity of the basis and penalty and the ability to mix-and-match the orders of spline basis functions and penalties.

We denote the B-spline basis as of order $m_1$, and $m_1 = 3$ denotes a cubic spline. Associated with the spline will be a derivative based penalty
\begin{align*}
    P_{\lambda} = \lambda \int_{0}^{T}\boldsymbol{\beta}^{[m_2]}(t)^2dt
\end{align*}
where $\bbeta^{[m_2]}(t)$ denotes the $m_2^{th}$ derivative of $\bbeta$ with respect to $t$. It is assumed that $m_2 \leq m_1$, otherwise it makes no sense that the penalty is formulated in terms of a derivative that is not properly defined for the basis functions. Similarly, $P_{\lambda}$ can be written as $\boldsymbol{\theta}^T\boldsymbol{S}\boldsymbol{\theta}$  where $\bS$ is a banded diagonal matrix of known coefficients. The algebraic expression of $\bS$ is complex, as discussed in \textbf{citet{wood2017p}}. However, this has little impact on the computation time. 


###Usage
The usage of Newton Method with penalization `coxtp` is very similar as `coxtv` introduced above. The main difference comes from the penalization parameter selection.

We use the smooth spline here for penalization(default).
You could also use `spline="P-spline"`.
For lambda_spline, you could either enter a numeric number or a vector of numbers.
If `lambda_spline` was entered as a vector of numbers, the best lambda was selected based on different criteria(AIC, TIC or GIC).
Following is a model fit with the `lambda_spline` as a vector for different illustration purposes. We use the relatively harsh setting `ExampleDataBinary` to illustrate the useage of 'coxtp', the penalization method.

```{r}
data(ExampleDataBinary)
z     <- ExampleDataBinary$x
time  <- ExampleDataBinary$time
event <- ExampleDataBinary$event
```


```{r, message = FALSE, results='hide'}
lambda_spline_all=c(0.001,0.01,0.1,1,10,100,1000)
fit.pspline <- coxtp(event = event, z = z, time = time, lambda_spline=lambda_spline_all, spline = "P-spline")
fit.smoothspline <- coxtp(event = event, z = z, time = time, lambda_spline=lambda_spline_all, spline = "Smooth-spline")
```

```{r,  fig.ncol = 1, fig.show="hold", out.width="40%",out.height="44%", message = FALSE, warning=FALSE}
plot(fit.pspline, IC = "TIC", allinone = TRUE, ylim = c(-2,2))
plot(fit.smoothspline, IC = "TIC", allinone = TRUE, ylim = c(-2,2))
```


The optimal lambda was saved in the model term `lambda.selected`

```{r}
best_lambda=fit.pspline$lambda.selected
best_lambda
```

From the result above, we noticed that with different selection criteria, the best lambda selected is different.
Here, we use the AIC criteria which set `lambda=1000`.
The result for this model was saved in related criteria model and could be called as below:

<!-- ```{r} -->
<!-- AIC_model  = fit_penalized$model.AIC -->
<!-- summary(AIC_model) -->
<!-- ``` -->

Compare with the non-penalized Model, we could see that the effect of "V1" was shrink to roughly linear in the penalized model.

The detail calculation of B-spline matrix, $\theta$ matrix and $\beta$ matrix was similar as Newton's method.

### Baseline estimation

The Nelson-Aalen estimator (Breslow estimator) of the culmulative function is given by $\widetilde{\Lambda}(t) = \int_0^t \widetilde{\Lambda}_0(u)$, where $\widetilde{\Lambda}(t)$ is 0 except at the observed failure times $t_i$, where it takes the value \begin{align*}
    d\Lambda_0 = {d_i}\left\{\mathop{\sum}\limits_{\ell \in R(T_i)} \exp \{\boldsymbol{X}_{i' }^T \boldsymbol{\Theta}  \boldsymbol{B}(T_{i}) \}\right\}^{-1}.
\end{align*}

The baseline estimation here refers to the baseline hazard at time t when holding all the covariates equals to zero.
We use the model fitted result `fit_penalized` in section 4.2.

<!-- ```{r,fig.height = 7, fig.width = 7, fig.align = "center"} -->

<!-- event=sim_data[,"event"] -->
<!-- time=sim_data[,"time"] -->
<!-- data=sim_data[,!colnames(sim_data) %in% c("event","time")] -->


<!-- model1  = fit_penalized$model.AIC -->

<!-- ##baseline -->
<!-- plotdata=coxtp.baseline(fit=model1, delta=event,z=data,time=time) -->
<!-- head(plotdata) -->
<!-- ``` -->

<!-- Let's check the result of baseline function first. -->
<!-- The function returned a dataset containing the baseline estimation including 3 variables: -->

<!-- -   `unique.time.`: The unique time listed in the original dataset used to fit the model -->
<!-- -   `lambda`: refers to $\lambda_0(t)$ in the model, which is baseline hazard -->
<!-- -   `Lambda`: refers to the cumulative baseline hazard in the model. -->

<!-- Note that since our baseline estimation is based on Breslow Estimator, when there is no ties in the data, the baseline estimation could sometime results as 0 since there is no death at certain time-point. -->
<!-- Thus, if we directly draw the plot, it would be something like this: -->

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->

<!-- #Exclude censoring points -->

<!-- baseline_plot<-ggplot(plotdata,aes(x=unique.time., y=lambda)) + geom_line(size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis', limits=c(0,3), breaks=c(0,1,2,3)) + -->
<!--   scale_y_continuous(name='baseline hazard', limits=c(0,0.1)) + -->
<!--   ggtitle(" Baseline Hazard by time") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- baseline_plot -->

<!-- ``` -->

As a result, we could either increase the time interval to make ties exists or remove the points that have $\lambda=0$.
The only differences would result in the baseline hazard, there is no influence on the cumulative baseline hazard.

<!-- ```{r} -->
<!-- plotdata=plotdata[plotdata$lambda!=0,] -->
<!-- ``` -->

<!-- Next, we are going to visualize the result using `ggplot2` and `cowplot` function. -->

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->

<!-- #Exclude censoring points -->

<!-- baseline_plot<-ggplot(plotdata,aes(x=unique.time., y=lambda)) + geom_line(size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis', limits=c(0,3), breaks=c(0,1,2,3)) + -->
<!--   scale_y_continuous(name='baseline hazard', limits=c(0,0.1)) + -->
<!--   ggtitle(" Baseline Hazard by time") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- cum_plot<-ggplot(plotdata,aes(x=unique.time., y=Lambda)) + geom_line(size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis', limits=c(0,3), breaks=c(0,1,2,3)) + -->
<!--   scale_y_continuous(name='Cumulative baseline hazard') + -->
<!--   ggtitle("Cumulative Baseline Hazard by time") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- plot_grid(baseline_plot,cum_plot,ncol = 2) -->

<!-- ``` -->

One use of Cumulative baseline hazard is to calculate the survival function where cumulative hazard is the negative log of the survival probabilities.
If we assume cumulative baseline hazard function as $S(t)$ and cumulative hazard function as $H(t)$, then we have $H(t)=-log(S(t))$.
Thus, we could plot the survival function as follows:

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->
<!-- plotdata$survive=exp(-plotdata$Lambda) -->

<!-- ggplot(plotdata,aes(x=unique.time., y=survive)) + geom_line(size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis', limits=c(0,3), breaks=c(0,1,2,3)) + -->
<!--   scale_y_continuous(name='Survival Function') + -->
<!--   ggtitle("Survival by time") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- ``` -->

### Testing for time-varying effect

## 5.6 Model prediction:

### 5.6.1 Simple prediction

To predict the new data, we offered the function coxtp.predict.
This function could also be used to calculate absolute hazard.
Suppose the new data to be predicted is `c(1,1,0,0,0)` for `V1` to `V5`.
For data with no stratification: (Suppose we already have the best tuning parameter `lambda_spline` selected).

<!-- ```{r} -->
<!-- sim_data=sim_data_p5 -->
<!-- event=sim_data[,"event"] -->
<!-- time=sim_data[,"time"] -->
<!-- data=sim_data[,!colnames(sim_data) %in% c("event","time")] -->

<!-- lambda_spline=1000 -->
<!-- fit_penalized <- coxtp(event = event, z = data, time = time,lambda_spline=lambda_spline) -->

<!-- model_result  = fit_penalized$model_result -->
<!-- baseline=coxtp.baseline(fit=model_result, delta=event,z=data,time=time) -->
<!-- data_predict=c(1,1,0,0,0) -->
<!-- predict=coxtp.predict(model_result,baseline,newdata=data_predict) -->

<!-- ``` -->

<!-- The result dataset `predict` is a dataset with predicted hazard and time points in the orginal dataset. -->
<!-- We could plot the data using ggplot: -->

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->
<!-- ggplot(predict,aes(x=unique.time.,y=lambda0_exp_betax)) + -->
<!--   geom_line(size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis') + -->
<!--   scale_y_continuous(name='Predicted hazard') + -->
<!--   ggtitle("Predicted Hazard") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- ``` -->

### 5.6.2 absolute hazard

If we want to calculate and show the difference of absolute hazard for variable `V2`, we could use the `coxtp.predict` to get it done.
Assume that besides `V2`, other covariate are all set to the reference level.

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->
<!-- data_predict0=c(0,1,0,0,0) -->
<!-- predict0=coxtp.predict(model_result,baseline,newdata=data_predict0,strata = F) -->
<!-- data_predict1=c(0,0,0,0,0) -->
<!-- predict1=coxtp.predict(model_result,baseline,newdata=data_predict1,strata = F) -->

<!-- ggplot() + -->
<!--   geom_line(aes(x=predict0$unique.time.,y=predict0$lambda0_exp_betax,color="V2=0"),size = 0.6) + -->
<!--   geom_line(aes(x=predict1$unique.time.,y=predict1$lambda0_exp_betax,color="V2=1"),size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis') + -->
<!--   scale_y_continuous(name='Predicted hazard') + -->
<!--   ggtitle("Predicted Hazard") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- ``` -->

<!-- ### 5.6.3 Prediction for certain time points -->

<!-- Also, we could specify the time points of output by define `out_seq`. -->
<!-- Following is an example: -->

<!-- Let's check the output first: -->

<!-- ```{r} -->
<!-- predict_seq=coxtp.predict(model_result,baseline,newdata=data_predict,strata = F,out_seq=seq(0,3,by=0.5)) -->
<!-- predict_seq -->
<!-- ``` -->

<!-- Instead of outputing all the data points, the `coxtp.predict` will only output the time points that has been specified. -->
<!-- Next, we are going to see if there is any difference between the one with default output and the one with specified time points: -->

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->
<!-- predict_seq=coxtp.predict(model_result,baseline,newdata=data_predict,strata = F,out_seq=seq(0,3,by=0.1)) -->

<!-- plot_or=ggplot(predict,aes(x=unique.time.,y=lambda0_exp_betax)) + -->
<!--           geom_line(size = 0.6) + -->
<!--           scale_x_continuous(name='Years since diagnosis') + -->
<!--           scale_y_continuous(name='Predicted hazard') + -->
<!--           ggtitle("Without specify out_seq") + -->
<!--           theme(plot.title = element_text(hjust = 0.5)) -->

<!-- plot_seq=ggplot(predict_seq,aes(x=unique.time.,y=lambda0_exp_betax)) + -->
<!--           geom_line(size = 0.6) + -->
<!--           scale_x_continuous(name='Years since diagnosis') + -->
<!--           scale_y_continuous(name='Predicted hazard') + -->
<!--           ggtitle("With specify out_seq") + -->
<!--           theme(plot.title = element_text(hjust = 0.5)) -->
<!-- plot_grid(plot_or,plot_seq,ncol = 2) -->


<!-- ``` -->

From the result above, we could notice that only the function smoothness looks different.

<!-- ### 5.6.3 Prediction with stratification -->

For data with stratification, just define that `strata=T`(Default is FALSE):(Again, suppose we already have the best tuning parameter lambda selected)

<!-- ```{r} -->
<!-- event_stra=sim_data_p5_f5[,"event"] -->
<!-- time_stra=sim_data_p5_f5[,"time"] -->
<!-- data_stra=sim_data_p5_f5[,!colnames(sim_data_p5_f5) %in% c("event","time","facility")] -->
<!-- facility=sim_data_p5_f5[,"facility"] -->

<!-- #select best lambda -->
<!-- lambda_spline=1000 -->
<!-- fit_stra<-coxtp(event=event_stra,z=data_stra,time=time_stra,strata =facility,lambda_spline=lambda_spline) -->
<!-- model_result  = fit_stra$model_result -->
<!-- baseline_strata=coxtp.baseline(fit=model_result, delta=event_stra,z=data_stra,time=time_stra,strata = facility) -->
<!-- data_predict=c(1,1,0,0,0) -->
<!-- predict=coxtp.predict(model_result,baseline_strata,newdata=data_predict,strata = T) -->

<!-- ``` -->

Again, we could plot the result by ggplot:

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center",message=FALSE} -->
<!-- ggplot(predict,aes(x=unique.time.,y=lambda0_exp_betax)) + -->
<!--   geom_line(aes(color=strata,group=strata),size = 0.6) + -->
<!--   scale_x_continuous(name='Years since diagnosis') + -->
<!--   scale_y_continuous(name='Predicted hazard') + -->
<!--   ggtitle("Predicted Hazard") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->
<!-- ``` -->

# 6. Model performance

## 6.1 Internal comparison

### 6.1.1 Accurancy:

For the non-penalized method, the estimation is largely depended on the number of knots chosen in the B-Spline base function.
The example shown above is using the default knots, which is `nspline=8`.
However, the estimation could be largely different if we choose other knots.
Following is the code showing the performance of the estimation when choosing different knots:

<!-- Following is the code showing the performance of the estimation when choosing different knots: -->

<!-- ```{r} -->
<!-- sim_data=surtvep::sim_data -->
<!-- event=sim_data[,"event"] -->
<!-- time=sim_data[,"time"] -->
<!-- data=sim_data[,!colnames(sim_data) %in% c("event","time")] -->

<!-- knot_list=c(6,8,10,20) -->
<!-- labels=paste0("Knots=",knot_list) -->
<!-- x=seq(0,3,length.out=sum(event==1)) -->
<!-- y=1 -->
<!-- for(knot in knot_list){ -->
<!--   fit<-coxtp(event=event,z=data,time=time,nspline=knot) -->
<!--   plot<-coxtp.plot(fit,coef="V1",ylab="HR(log-scale)") + -->
<!--     theme(text = element_text(size = 10), -->
<!--           axis.text.x = element_text(size = 8), -->
<!--           axis.text.y=element_text(size = 8)) + -->
<!--     ggtitle("") + geom_line(aes(x=x,y=y)) -->
<!--   plot -->
<!--   assign(paste0("plot",knot),plot) -->
<!-- } -->

<!-- library(cowplot) -->


<!-- title <- ggdraw() + -->
<!--   draw_label( -->
<!--     "Non-penalized NR", -->
<!--     fontface = 'bold', -->
<!--     x = 0, -->
<!--     hjust = 0, -->
<!--     size=15 -->
<!--   ) + -->
<!--   theme( -->
<!--     plot.margin = margin(0, 0, 0, 7) -->
<!--   ) -->

<!-- ``` -->

<!-- ```{r,fig.height = 7, fig.width = 7, fig.align = "center"} -->

<!-- p1=plot_grid(plot6,plot8,plot10,plot20,ncol=2,labels=c(labels),label_size = 13) -->
<!-- p1 -->
<!-- ``` -->

<!-- For the result above, the red line and shadow refer to the estimated function and 95%CI while the black line refers to the true function which is time-fixed(y=1). -->
<!-- From the result above, we noticed that as the knots increase, the estimation becomes more and more inaccurate. -->
<!-- This is happening because as the number of knots increases, the variance in the data was mistakenly captured as its influence. -->
<!-- This makes the number of knots of selection in the non-penalized model very important. -->

<!-- While for penalized model, since we have penalized term, the number of knots didn't matters much, instead, the $\lambda$ is more important. -->
<!-- Following plot is the performance of estimation when we use the lambda_spline selected before, which is `lambda_spline= 1000` -->

<!-- ```{r} -->
<!-- knot_list=c(6,8,10,20) -->
<!-- labels=paste0("Knots=",knot_list) -->
<!-- x=seq(0,3,length.out=sum(event==1)) -->
<!-- y=1 -->
<!-- for(knot in knot_list){ -->
<!--   fit<-coxtp(event=event,z=data,time=time,nspline=knot,lambda_spline = 1000) -->
<!--   plot<-coxtp.plot(fit,coef="V1",ylab="HR(log-scale)") + -->
<!--     theme(text = element_text(size = 10), -->
<!--           axis.text.x = element_text(size = 8), -->
<!--           axis.text.y=element_text(size = 8)) + -->
<!--     ggtitle("") + -->
<!--     geom_line(aes(x=x,y=y)) -->
<!--   plot -->
<!--   assign(paste0("plot",knot),plot) -->
<!-- } -->

<!-- ``` -->

<!-- ```{r,fig.height = 7, fig.width = 7, fig.align = "center"} -->
<!-- library(cowplot) -->

<!-- title <- ggdraw() + -->
<!--   draw_label( -->
<!--     paste0("Penalized NR,lambda=",1000), -->
<!--     fontface = 'bold', -->
<!--     x = 0, -->
<!--     hjust = 0, -->
<!--     size=15 -->
<!--   ) + -->
<!--   theme( -->
<!--     plot.margin = margin(0, 0, 0, 7) -->
<!--   ) -->
<!-- p1=plot_grid(plot6,plot8,plot10,plot20,ncol=2,labels=c(labels),label_size = 13) -->
<!-- p1 -->

<!-- ``` -->

From the plot above, we could observe that with different knots selected, the performance is not different so much for different knots, all the estimates are relatively close to the real function.
However, in the penalized model, the lambda is more important which is another reason that we need to select the best lambda before actually fitting the model.
The following plot is the performance of different lambda selected.
For the following plot, the knot was set as default, which is `knot=8`.

<!-- ```{r} -->
<!-- lambda_spline_all=c(0.001,0.01,0.1,1,10,100) -->
<!-- labels=paste0("lambda=",lambda_spline_all) -->
<!-- i=1 -->
<!-- x=seq(0,3,length.out=sum(event==1)) -->
<!-- y=1 -->
<!-- for(lambda in lambda_spline_all){ -->
<!--   fit<-coxtp(event=event,z=data,time=time,lambda_spline = lambda) -->
<!--   plot<-coxtp.plot(fit,coef="V1",ylab="HR(log-scale)") + -->
<!--     theme(text = element_text(size = 10), -->
<!--           axis.text.x = element_text(size = 8), -->
<!--           axis.text.y=element_text(size = 8)) + -->
<!--     ggtitle("")  + -->
<!--     geom_line(aes(x=x,y=y)) -->
<!--   plot -->
<!--   assign(paste0("plot",i),plot) -->
<!--   i=i+1 -->
<!-- } -->

<!-- library(cowplot) -->

<!-- title <- ggdraw() + -->
<!--   draw_label( -->
<!--     paste0("Penalized NR,knot=",8), -->
<!--     fontface = 'bold', -->
<!--     x = 0, -->
<!--     hjust = 0, -->
<!--     size=15 -->
<!--   ) + -->
<!--   theme( -->
<!--     plot.margin = margin(0, 0, 0, 7) -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r,fig.height = 9, fig.width = 7, fig.align = "center"} -->
<!-- p1=plot_grid(plot1,plot2,plot3,plot4,plot5,plot6,ncol=2,labels=labels,label_size = 13) -->
<!-- p1 -->

<!-- ``` -->

<!-- As a result, as the lambda increase, the estimated function is more related to the true function. -->
<!-- This is happening because that the true function is time-fixed function, thus, as the lambda increase, the real function tends to shrink to the fixed function. -->

<!-- ### 6.1.2 Efficacy -->

<!-- Next, we are going to compare the Efficacy of different methods. -->
<!-- The main function we used to eliminate computation time by parallel. -->
<!-- For the function, the default setting for parallel is `parallel=FALSE`. -->
<!-- For parallel computation, we also need to define `threads`, which refers to the number of cores of the computer. -->
<!-- Here we set `thread=4`. -->
<!-- The following code and plot is to use to compare parallel and non-parallel time for different sample size: -->

<!-- ```{r} -->
<!-- library(stringi) -->
<!-- library(stringr) -->
<!-- library(dplyr) -->
<!-- samplesize_list=c(1000,2000,3000,4000,5000) -->
<!-- i=1 -->
<!-- for(samplesize in samplesize_list){ -->
<!--   print(i) -->
<!--   sim_data_sub=sim_data[1:samplesize,] -->
<!--   event_sub=sim_data_sub[,"event"] -->
<!--   time_sub=sim_data_sub[,"time"] -->
<!--   data_sub=sim_data_sub[,!colnames(sim_data_sub) %in% c("event","time")] -->

<!--   non_parallel=bench::mark(coxtp(event=event_sub,z=data_sub,time=time_sub,lambda_spline = 100))[,1:8] -->
<!--   parallel=bench::mark(coxtp(event=event_sub,z=data_sub,time=time_sub,lambda_spline = 100,parallel = T,threads = 4))[,1:8] -->
<!--   non_parallel[,1]=samplesize -->
<!--   parallel[,1]=samplesize -->

<!--   if(i!=1){ -->
<!--     non_parallel_list=rbind(non_parallel_list,non_parallel) -->
<!--     parallel_list=rbind(parallel_list,parallel) -->
<!--   } else { -->
<!--     non_parallel_list=non_parallel -->
<!--     parallel_list=parallel -->
<!--   } -->

<!--   i=i+1 -->
<!-- } -->

<!-- parallel_list$parallel="Parallel" -->
<!-- non_parallel_list$parallel="None-Parallel" -->
<!-- time_data=rbind(parallel_list,non_parallel_list) %>% -->
<!--   mutate(median_time=str_remove(median,"s"), -->
<!--          median_time=str_remove(median_time,"m"), -->
<!--          median_time=as.numeric(median_time), -->
<!--          median_time=ifelse(str_detect(median,"ms"),median_time/1000,median_time)) %>% -->
<!--   rename(samplesize=expression) -->
<!-- #Plot Time used: -->
<!-- ``` -->

<!-- ```{r,fig.height = 5, fig.width = 7, fig.align = "center"} -->

<!-- ggplot(time_data,aes(x=samplesize,y=median_time,group=parallel)) + -->
<!--   geom_point(aes(color=parallel)) + -->
<!--   geom_line(aes(color=parallel)) + -->
<!--   labs(x="Sample Size",y="Seconds",title="Internal Computation time Comparison") -->

<!-- ``` -->

<!-- ## 6.2 External comparison -->

<!-- ### 6.2.1 Accurancy -->

<!-- In this section, we are going to compare the performance between our package and other time-varying survival packages. -->

<!-- # 7. Simulation -->

<!-- In this section, we will generate our model prediction to large dataset and illustrate how to do simulation based on our package -->

<!-- To start with, we first divided our dataset into training and testing dataset: -->

<!-- ```{r} -->
<!-- sim_data=sim_data_p5 -->
<!-- #Generate ID for sampling -->
<!-- sim_data=cbind(sim_data,c(1:nrow(sim_data))) -->
<!-- train_id=sample(c(1:nrow(sim_data)),2000,replace=F) -->
<!-- train_data=sim_data[sim_data[,8] %in% train_id, ] -->
<!-- test_data=sim_data[!sim_data[,8] %in% train_id, ] -->
<!-- #Remove ID -->
<!-- train_data=train_data[,1:7] -->
<!-- test_data=test_data[,1:7] -->
<!-- ``` -->

<!-- Next, we are going to build the model using the training data. -->
<!-- The step is similar as we have shown in previous step -->

<!-- ```{r} -->
<!-- event=train_data[,"event"] -->
<!-- time=train_data[,"time"] -->
<!-- data=train_data[,!colnames(train_data) %in% c("event","time")] -->
<!-- lambda_spline_all=c(0.001,0.01,0.1,1,10,100,1000) -->
<!-- fit_penalized <- coxtp(event = event, z = data, time = time,lambda_spline=lambda_spline_all) -->
<!-- fit_penalized$lambda.selected -->

<!-- ``` -->

<!-- From the result above, we noticed that the lambda.selected is 1. -->
<!-- Next, we are going to predict the model using the test_data. -->
<!-- Here we are going to treat the `test_data` as a whole new population data and want to predict the event probability by time. -->

<!-- From previous illustration, we know that the function `cox.predict` could generate absolute hazard for one individual during the time period. -->


## Real Data Example `SUPPORT`

In this section, we illustrate the usage of our package by analyzing the time-varying effects on the real data `SUPPORT` (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment).

Some of the original data was missing. Before imputation, there were a total of 9105 individuals and 47 variables. Of those variables, a few were removed before imputation. We removed three response variables:
hospital charges, patient ratio of costs to charge,s and patient micro-costs. Next, we removed hospital death as it was directly informative of our event of interest, namely death. We also removed functional disability and income as they are ordinal covariates. Finally, we removed 8 covariates related to the results of previous findings: we removed SUPPORT day 3 physiology score (\code{sps}), APACHE III day 3 physiology score (\code{aps}), SUPPORT model 2-month survival estimate, SUPPORT model
6-month survival estimate, Physician's 2-month survival estimate for pt., Physician's 6-month survival estimate for pt., Patient had Do Not Resuscitate (DNR) order, and Day of DNR order (<0 if before study). Of these, \code{sps} and \code{aps} were added on after imputation, as they were missing only 1 observation. First we imputed manually using the normal values for physiological measures recommended by Knaus et al. (1995). Next, we imputed a single dataset using \pkg{mice} with default settings. After imputation, we noted that the covariate for surrogate activities of daily living was not imputed. This is due to collinearity between the other two covariates for activities of daily living. Therefore, surrogate activities of daily living was removed.

First we load the data:

```{r, message = FALSE}
data(support)
# Using the matrix interface and log of time
z <- model.matrix(death ~ . - d.time - 1, data = support)
event <- support$death
time <- support$d.time
```

We apply the Newton's method with Smoothing-spline:
```{r, message = FALSE, results='hide', fig = TRUE, fig.width=7, fig.height=3.5, warning = FALSE}
fit.support <- coxtv(z = z, event = event, time=time)
```



